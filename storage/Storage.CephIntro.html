<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/colorful.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>CephIntro - Uxun's WIKI</title>
    <meta name="keywords" content=""/>
    <meta name="description" content=""/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
  </head>

  <body>
    <div id="container" class="typo">
      
<div id="header">
  <div id="post-nav">
    <a href="/wiki/">Uxun's WIKI</a>
    &nbsp;&#187;&nbsp;
    <a href="/wiki/#../../../Dropbox/wiki/storage">../../../Dropbox/wiki/storage</a>
    &nbsp;&#187;&nbsp;CephIntro
    <span class="updated">Page Updated&nbsp;
    2019-08-21 15:00
    </span>
  </div>
</div>
<div class="clearfix"></div>
<div id="content">
  <h2 id="intro-to-ceph">Intro to Ceph</h2>
<blockquote>
<p>可靠的自主分布式对象存储（<a href="https://docs.ceph.com/docs/master/glossary/#term-rados">RADOS</a>）</p>
<p>一个Ceph存储集群要求至少有一个Ceph监视器和两个Ceph OSD守护进程。当运行Ceph文件系统客户端时，必须要有Ceph元数据服务器。</p>
</blockquote>
<h3 id="1archiecture">1.<a href="https://docs.ceph.com/docs/master/architecture/#architecture">Archiecture</a></h3>
<p><img alt="" src="https://docs.ceph.com/docs/master/_images/stack.png" /></p>
<ol>
<li>
<p>提供对象，块和文件存储。</p>
<blockquote>
<p>块存储：读写快，不利于共享<br />
文件存储：读写慢，利于共享<br />
对象存储：读写快，利于共享</p>
</blockquote>
</li>
<li>
<p>集群至少需要一个：</p>
<blockquote>
<ul>
<li>Monitors (ceph-mon): 监视集群状态，维护集群(Monitor,Manager,OSD,CRUSH)映射,管理身份验证。HA ceph-mon &gt; 3</li>
<li>Manager (ceph-mgr): 负责跟踪运行时指标和Ceph集群的当前(存储利用率，性能，负载)状态，托管REST API。HA ceph-mgr &gt; 2</li>
<li>OSDs (ceph-osd): 对象存储守护进程：存储数据，处理数据复制，恢复，平衡osd，获取心跳信息为ceph-mon和cph-mgr提供些监视信息。HA ceph-osd &gt; 3</li>
<li>MDS (ceph-mds): 代表<a href="https://docs.ceph.com/docs/master/glossary/#term-ceph-filesystem">Ceph文件系统</a>存储元数据，允许POSIX文件系统的用户执行命令。(对象，块不使用)</li>
</ul>
</blockquote>
</li>
<li>
<p>Flow: obj() —&gt; File() —&gt; Disk()<br />
    obj(数据存储为对象) —&gt; File(对象应用于文件系统中的文件) —&gt; Disk(该文件存储在对象存储设备上)</p>
</li>
<li>
<p>使用 <a href="https://docs.ceph.com/docs/master/glossary/#term-crush">CRUSH</a>算法，Ceph计算应该包含对象的放置组，并进一步计算哪个Ceph OSD守护进程应该存储放置组。CRUSH算法使Ceph存储集群能够动态扩展，重新平衡和恢复。</p>
</li>
</ol>
<blockquote>
<p>Ceph OSD守护程序作为一个心跳向Ceph的监视器报告一些检测信息。Ceph的存储集群需要至少2个OSD守护进程来保持一个 active + clean状态。（Ceph默认制作2个备份，但你可以调整它）</p>
<p>Monitors:Ceph的监控保持集群状态映射，包括OSD(守护进程)映射,分组(PG)映射，和CRUSH映射。 Ceph 保持一个在Ceph监视器, Ceph OSD 守护进程和 PG的每个状态改变的历史（称之为“epoch”）<br />
MDS: MDS是Ceph的元数据服务器，代表存储元数据的Ceph文件系统（即Ceph的块设备和Ceph的对象存储不使用MDS）。Ceph的元数据服务器使用POSIX文件系统，用户可以执行基本命令如 ls, find,等，并且不需要在Ceph的存储集群上造成巨大的负载。</p>
<p>Ceph把客户端的数据以对象的形式存储到了存储池里。利用CRUSH算法，Ceph可以计算出安置组所包含的对象，并能进一步计算出Ceph OSD集合所存储的安置组。CRUSH算法能够使Ceph存储集群拥有动态改变大小、再平衡和数据恢复的能力。</p>
</blockquote>
<h3 id="2ceph-api">2.Ceph API</h3>
<ul>
<li>Object：有原生的API，而且也兼容Swift和S3的API </li>
<li>Block：支持精简配置、快照、克隆 </li>
<li>File：Posix接口，支持快照 </li>
</ul>
<p><strong>Ceph也是分布式存储系统，它的特点是：</strong></p>
<p>高扩展性：使用普通x86服务器，支持10~1000台服务器，支持TB到PB级的扩展。 <br />
高可靠性：没有单点故障，多数据副本，自动管理，自动修复。 <br />
高性能：数据分布均衡，并行化度高。对于objects storage和block storage,不需要元数据服务器。</p>
<h3 id="3other">3.Other</h3>
<h3 id="virtualization-for-block"><a href="https://docs.ceph.com/docs/master/install/install-vm-cloud/">Virtualization for Block</a></h3>
<blockquote>
<p>如果您打算将Ceph Block Devices和Ceph Storage Cluster用作虚拟机（VM）或<a href="https://docs.ceph.com/docs/master/glossary/#term-cloud-platforms">云平台</a>的后端， 则QEMU / KVM和 <code>libvirt</code>软件包对于启用VM和云平台非常重要。VM的示例包括：QEMU / KVM，XEN，VMWare，LXC，VirtualBox等。云平台的示例包括OpenStack，CloudStack，OpenNebula等。</p>
</blockquote>
<h3 id="4recommendations">4.RECOMMENDATIONS</h3>
<p>REFERNECE <a href="https://docs.ceph.com/docs/master/start/hardware-recommendations/#production-cluster-examples">production-cluster-examples</a></p>
<p>1.CPU </p>
<div class="hlcode"><pre><span class="c"># Ceph-mds cpu密集型，动态分配负载。</span>
<span class="c"># Ceph-osd cpu密集型，运行RADOS服务，使用CRUSH计算数据放置，复制数据，维护集群映射副本。</span>
<span class="c"># Ceph-mon 只维护集群的主副本，对CPU需求不高。</span>
</pre></div>


<p>2.RAM</p>
<div class="hlcode"><pre><span class="n">ceph</span><span class="o">-</span><span class="n">mon</span> <span class="n">or</span> <span class="n">ceph</span><span class="o">-</span><span class="n">mgr</span> 
<span class="cp"># 根据集群大小设置，小型集群官方建议1-2G，大型5-10G。通过设置 mon_osd_cache_size或 rocksdb_cache_size </span>

<span class="n">ceph</span><span class="o">-</span><span class="n">mds</span> 
<span class="cp"># 内存利用率取决于缓存配置为消耗多少内存，mds_cache_memory</span>

<span class="n">ceph</span><span class="o">-</span><span class="n">osd</span>
<span class="cp"># 通常与设置的PG数量相关，osd_memory_target使用BlueStore时候后端osd 3-5G RAM</span>
</pre></div>


<p>3.<a href="https://docs.ceph.com/docs/master/start/hardware-recommendations/#data-storage">DATA</a></p>
<p>确定好你的ceph数据存储维度 (成本和性能权衡)。存储驱动器容量越大，每个Ceph OSD守护进程需要的内存越多，特别是在重新平衡，回填和恢复期间。一般的经验法则是1GB的存储空间大约1GB的RAM。</p>
<p>4.NET</p>
<p>每台主机至少有两个1Gbps网络接口控制器（NIC）</p>
<p><strong>OS建议</strong></p>
<blockquote>
<p>Ceph Version &gt; Jewel（v10.2.0）<br />
从Ceph 10.x（Jewel）开始，你应该使用至少一个4.x内核。如果您必须使用较旧的内核，则应使用fuse客户端而不是内核客户端。</p>
<p>对于RBD，如果您选择跟踪长期内核，我们目前推荐基于4.x的“长期维护”内核系列：</p>
<ul>
<li>4.19.z</li>
<li>4.14.z</li>
</ul>
<p><a href="https://docs.ceph.com/docs/master/start/os-recommendations/#platforms">系统版本</a></p>
<p><a href="https://docs.ceph.com/docs/master/cephfs/best-practices/#cephfs-best-practices">CEPHFS BEST PRACTICES</a></p>
</blockquote>
  
</div>
    </div>
    <div id="footer">
      <div class="footer-left">
        <p>
        Copyright © 2019 Uxun.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
        Theme by <a href="https://github.com/tankywoo/yasimple_x2" target="_blank">YASimple_X2</a>.
        </p>
      </div> <!-- end footer-left -->
      <div class="footer-right">
        <p>Page Updated 2019-08-27 18:21:48</p>
      </div> <!-- end footer-right -->
    </div>

    
    

  </body>
</html>